---
layout: default
title: "Machine Learning: With Math"
permalink: /machine-learning-math/
---
# Table of Contents
- [Neural Networks](#neural-networks)
    - [Thinking](#thinking)
        - [Feed-Forward](#feed-forward)
        - [Multiple Layers](#multiple-layers)
    - [Learning](#learning)
        - [Backpropagation](#backpropagation)
        - [Iterating Backwards](#iterating-backwards)
    - [Neural Networks Conclusion](#neural-networks-conclusion)
- [Transformers](#transformers)
    - [Words to Numbers](#words-to-numbers)
        - [Embedding Vectors](#embedding-vectors)
    - [Pay Attention](#pay-attention)
        - [Query, Key, and Value Matrices](#query-key-and-value-matrices)
        - [Final Attention Formula](#final-attention-formula)
        - [Multi-Headed Attention](#multi-headed-attention)
    - [Knowing Facts](#knowing-facts)
        - [The Universe of Facts](#the-universe-of-facts)
    - [How Transformers Learn](#how-transformers-learn)
        - [Predicting a Token](#predicting-a-token)
        - [Cost Function](#cost-function)
    - [Transformers Conclusion](#transformers-conclusion)
        - [Dimensions of the Semantic Universe](#dimensions-of-the-semantic-universe)
        - [Matrices and Dimensions Summary](#matrices-and-dimensions-summary)

# Neural Networks
Neural networks were one of the first examples of machine learning (besides maybe linear regression), and serves as a foundation for most modern models. What makes this data structure "neural" is its flexibility. A neural net is initialized with random[^1], arbitrary parameters which can be adjusted through applying a learning process on training data. The edges (represented as lines) connecting every pair of neurons (represented as circles) in adjacent layers allow information to be passed to and interact with each other. In the diagram below, while there are four neurons in the first layer and two neurons in the last layer, the size and number of layers can vary depending on the application.

![Neural Network Diagram](/assets/images/neural_network.png)  
*(Image credit: [Victor Zhou – Neural Networks from Scratch](https://victorzhou.com/series/neural-networks-from-scratch/))*

## Thinking
*"Machine thinking is just linear algebra on steroids." - Someone*  
Before getting into how a neural net learns, let's first discuss how it thinks. In the diagram above, imagine each edge connecting pairs of neurons as having a value associated with it, which we call its weight. Before the neural net looks at anything, the neurons themselves are empty, but they have an offset value, called its bias. These terms should become more clear as I explain the thinking process, which is called feed-forward.
### Feed-Forward
Now we are ready to give the neural net something to think about. In this section, I will use the classic example of finding recommended YouTube videos. For this application, the input layer might consist of a neuron for every YouTube video, with a value corresponding to how recently it was watched. The diagram above uses 4 input neurons, but for our case, we might use a lot more neurons to count every video. Next, for every neuron in the next layer, we calculate its actviation by summing the products of every value in the input layer and the associated weight of the edge that connects it to the next neuron.  
That was a lot of words! But if you think carefully about the math process actually happening here, this is essentially a matrix mutliplication, or a linear transformation. Once we get the activations of every neuron in the next layer, we offset each value by the corresponding bias. Finally, each value is sent through a nonlinear normalization function, such as sigmoid or ReLU. The purpose of this nonlinear function is to keep activations meaningful and improve stability. The choice of function depends on the application and each function has its benefits and drawbacks. (Data scientists have a lot more to say about this, but I will move on) The function used in each layer can also be different. For some applications, such as a handwritten digit recognizer, the softmax function is used in the output layer to convert the activations to a probability distribution.  
Nonlinear functions also bring in more flexibility by breaking properties of linear transformations. You may have thought that using an input size smaller than the hidden layer size, as shown in the diagram, is wasteful since the rank of the first linear transformation can be at most the size of the input layer. By applying nonlinear normalization functions to the first hidden layer, we allow the dimension of the transformation image to become nonlinear and potentially have more dimensions.
### Multiple Layers
We can now repeat this process for the next layers, until we reach the last layer, the output layer. The final values in the output layer determine what the neural net "thinks" about its input. In our case of handrwitten digits, we would use 10 neurons, each with an activation that represents how close the input is to that number. This process is essentially a repeated sequence of a linear transformation, a translation, then a nonlinear normalization function. Here's the full formula for the activation of each neuron:  
$${a_l}^i=N(b_l^i+\sum_{j=1}^n{a_{l-1}^j}w_{l-1}^{ij})$$  
Where $${a_l}^i$$ is the activation of neuron $$i$$ in layer $$l$$, $$N$$ is the nonlinear normalization function, $$n$$ is the size of the current layer, $${w_l}^{ij}$$ is the weight of the edge connecting layer $$l$$ neuron $$i$$ to layer $$l+1$$ neuron $$j$$[^2], and $$b_l^i$$ is the bias for layer $$l$$ neuron $$i$$. (superscripts here are just indices, not exponents)  
You might be wondering why we use multiple layers. The idea behind the neural network is that each layer should recognize some pattern, maybe a straight line or a curve, then recognize compounded patterns in later layers, maybe a loop or intersection. Using nonlinear normalization functions on each layer allows the neural net to focus on these patterns more. I say in theory because the patterns a neural net actually learns is very hard to deduce. There are actually many ways a machine can learn to recognize the number 4, most of which are not recognizable to us.
## Learning
*"Machine learning is just multivariable calculus on steroids." - Someone*  
As I mentioned before, all parameters, meaning the weights and biases, are initialized to random values. As you would expect, a newborn neural net would not make very accrurate predictions - it is literally blind guessing. We want a way to shift those weights and biases to make the predictions more accurate.
### Defining the Function
The first step is designing a mathematical function to tell the neural net how "badly" it predicted. We call this the cost function. For our example, if we input a vector representation of someone who watched a lot of Minecraft videos recently, we should see the high activations in the neurons corresponding to Minecraft-related videos. Test cases can be collected by finding a user's watch history and what video they decided to watch next, and use this as an input-expected data pair. We therefore can define the cost function as the mean squared error between the predicted value and expected value[^3]. Formally, it is denoted as:  
$$C=\sum_{i=1}^n{(a_l^i-y^i)^2}$$  
Where $$n$$ is the size of the output layer, $$a_l^i$$ is the predicted activation of neuron $$i$$, and $$y^i$$ is the expected activation of neuron $$i$$. With a well-defined function, we can compute the gradient derivative to find which direction will decrease the cost function the fastest. Think of it like finding the fastest descent to the bottom of a high-dimensional elevation field. However, is a problem with directly computing the derivative. Notice how the activation of each output neuron is a sum of functions of previous activations, which in turn are also sums of functions of previous activations. Trying to expand out these sums will become exponentially massive. Not remotely practical for deep neural nets. Instead, we use an algorithm called backpropagation, which allows us to iteratively apply the chain rule and optimize the computation.
### Backpropagation
![Simplified Neural Network](/assets/images/nn_simple.png)  
To illustrate backpropagation, let's simplify our neural network to just three layers (1 input, 1 hidden, 1 output), with just one neuron in each layer. Since we are temporarily ignoring the presence of multiple neurons in a layer, we do not need the superscript yet.  Focusing on just the last two layers, let's first compute the derivative of the cost function with respect to the activation of the output neuron:  
$$\frac{\delta C}{\delta a_l}=2(a_l-y)$$  
While we cannot directly change an activation, this formula will be important to determine the deriatives of other parameters that can induce a change in the activation, thus changing the cost: change the weight of the edge connecting the previous neuron to the output, change the bias applied to the output neuron, or indirectly change the activation of the previous neuron. We can apply the chain rule to compute the derivative of the cost function with respect to each of these variables:  
$$\frac{\delta C}{\delta w_{l-1}}=2(a_l-y)\frac{\delta a_l}{\delta w_{l-1}}$$  
$$\frac{\delta C}{\delta b_l}=2(a_l-y)\frac{\delta a_l}{\delta b_l}$$  
$$\frac{\delta C}{\delta a_{l-1}}=2(a_l-y)\frac{\delta a_l}{\delta a_{l-1}}$$  
As you can see, all the derivatives are scaled by how far away the output neuron activation is from the expected value. That implies that changing outputs that were more badly predicted has a greater impact on the cost. This should make sense. Let's compute the derivative of the output neuron activation with respect to the edge weight:  
$$\frac{\delta a_l}{\delta w_{l-1}}=N'(b_l+a_{l-1}w_{l-1})\frac{\delta}{\delta w_{l-1}}(b_l+a_{l-1}w_{l-1})$$  
$$=N'(b_l+a_{l-1}w_{l-1})a_{l-1}$$  
Next, derivative of the output neuron activation with respect to the bias:  
$$\frac{\delta a_l}{\delta b_l}=N'(b_l+a_{l-1}w_{l-1})\frac{\delta}{\delta b_l}(b_l+a_{l-1}w_{l-1})$$  
$$=N'(b_l+a_{l-1}w_{l-1})$$  
Finally, derivative of the output neuron activation with respect to the activation of the previous neuron:  
$$\frac{\delta a_l}{\delta a_{l-1}}=N'(b_l+a_{l-1}w_{l-1})\frac{\delta}{\delta a_{l-1}}(b_l+a_{l-1}w_{l-1})$$  
$$=N'(b_l+a_{l-1}w_{l-1})w_{l-1}$$
### Iterating Backwards
Remember, we cannot use this derivative to directly change the activation of the previous neuron. Instead, we iterate to the previous layer recomputing the partial derivatives for the that layer, using the value of $$\frac{\delta C}{\delta a_{l-1}}=2(a_l-y)\frac{\delta a_l}{\delta a_{l-1}}$$ which we have now computed. Keep iterating backwards until we reach the input layer. At this point, we should have a derivative for every weight and bias calculated. This is our (massive) gradient derivative.  
The partial derivatives for a full neural net with multiple training data points are actually not much more complicated. We simply need to throw the summations and superscripts back in. You can verify these expressions on your own:  
$$\frac{\delta C}{\delta a_l^i}=2(a_l^i-y^i)$$  
$$\frac{\delta C}{\delta w_{l-1}^{ij}}=2(a_l^i-y^i)N'(b_l+\sum_{j=1}^{n}a_{l-1}^iw_{l-1}^{ij})a_{l-1}^j$$  
$$\frac{\delta C}{\delta b_l^i}=2(a_l^i-y^i)N'(b_l^i+\sum_{j=1}^{n}a_{l-1}^iw_{l-1}^{ij})$$  
$$\frac{\delta C}{\delta a_{l-1}^j}=\sum_{i=1}^{n}2(a_l^i-y^i)N'(b_l^i+\sum_{j=1}^{n}a_{l-1}^iw_{l-1}^{ij})w_{l-1}^{ij}$$  
The main thing to note here is that for every neuron activation in a layer, its activation will be influenced by every neuron activation in the previous layer, so we need to sum up those derivatives to compute $$\frac{\delta C}{\delta a_{l-1}}$$. For multiple training data points, simply sum all[^4] the gradients together to get the average change to optimally fit all outputs. With the full gradient derivative computed, we now shift every weight and bias by a tiny fraction of its corresponding derivative value. This should inch us slightly closer to a better predictor. There's more to say about how parameters get updated for optimal long-term improvement, but after repeating this process over and over on the training data, we should have a pretty solid YouTube video recommendation algorithm[^5].
## Neural Networks Conclusion
With neural networks, we have a unique data structure with flexible parameters, allowing it to fit to any evaluation application. The combination of linear transformations and nonlinear normalization functions preserve both computational simplicity and the ability to recognize nonlinear patterns. Having a well-defined function for output values allows a derivative gradient to be calculated, illuminating the necessary changes to adjust the parameters to better fit training data. However, it is essential to manually analyze the model to find optimal the network design.
# Transformers
While neural networks are great at multiple choice, (evaluation tasks such as recognizing handwritten digits) they struggle to generate content. Transformers were proposed to solve this task. By predicting the next word, pixel, or wave (we will call these segments of content "tokens") to come next, transformers give the illusion of original generation. They are used for modern tasks such as natural language processing, computer vision, reinforcement learning, and other tasks that are simply too complex for neural networks alone to be practical. A popular example is ChatGPT (GPT stands for Generative Pre-trained Transformer), which can mimic conversations using data from the internet.  
Interestingly, transformers were not the first machine learning architecture proposed for these tasks. Other models, such as recurrent neural networks, have been used, but transformers are have higher performance and more parallelizable, cutting training and computing time cost. Transformers use multiple layers to allow for deeper learning, just like neural networks, but what these layers do is more complex. In this section, I will mainly base my discussion around the model that powers ChatGPT as it is a very popular example of a transformer.
## Words to Numbers
Computers love programming languages, but have a hard time understanding natural languages. Language is imprecise, has culture weaved into it, and can change meaning depending on the context. The word "queen", for example, can represent a royal figure or a chess piece. Adjectives before nouns can also drastically change meaning. In short, it is difficult, if not impossible, to convert natural language to a language computers can understand.
### Embedding Vectors
Nevertheless, we can approximate meanings. We want to convert words to tokens[^6], which have associated embedding vectors. These vectors have very high dimension (>10,000 in GPT-3) because we want to be able to somehow capture the entire semantic universe into a list of numbers. Imagine a high-dimensional space (which I like to call the semantic universe) where each axis represents an idea. Maybe one axis is the "color" axis, where embedding vectors of words that are associated with different colors have different entries in that dimensions. Another axis could be the "temperature" axis, another being the "agreeable" axis, and so on, until we can approximate every point in the semantic universe with a vector. However, just like how the patterns a neural network learns is rarely understandable to humans, the meanings of each dimension that a transformer learns usually does not makes sense.  
Another property of these embedding vectors is that vectors can be added together to represent compounded ideas. Suppose $$v_1$$ represents the embedding vector associated with the word "mother", $$v_2$$ represents "father", $$v_3$$ represents "uncle", and $$v_4$$ represents "aunt". $$v_1-v_2$$, the difference between "mother" and "father", is very close to $$v_3-v_4$$, the difference between "uncle" and "aunt". This suggests that the vector $$v_1-v_2$$ can somehow convert masculine words to their feminine counterpart when added to an embedding vector. It also suggests that our complex semantic universe, despite having far more dimensions than we can visualize, is somehow quantifiable with linear algebra? And how could a mere 10,000 dimensions possibly achieve this? I found this strange and slightly unsettling when I learned about it. I'll return to this point at the end of the section.
## Pay Attention
A major difficulty with conversational bots is its ability to remember the past. Transformers take advantage of a revolutionary algorithm to remember context proposed by the paper *Attention is All You Need*, called attention. While recurrent neural networks were able to remember context, its processing mechanism was sequential, unable to be parallelized, and had very unstable gradients in the training process. Attention mitigates these issues, making it faster and more accurate for larger context sizes.  
Visualize the input as a list of embedding vectors, representing a text sequence. For example, there might be 8 vectors representing the incomplete sentence "The quick brown fox jumps over the lazy". Attention assumes that every token could have some sort of influence on every other token, so we want to compute what those influences are and how to adjust the vectors to include its more refined contextual meaning. We create an nxn attention matrix to compare every pair of tokens. We then use three matrices to determine how their embedded vector entries should be updated.
### Query, Key, and Value Matrices
The first of these matrices is the query matrix. This matrix is multiplied to every vector to produce a list of query vectors. Think of query vectors as a question that we want the corresponding token to ask of its context. In the example sentence, the token "fox" might have the query "are there any adjectives that change my definition?" The query matrix should be learned to produce these useful questions when applied to embedded vectors. Like with most machine-learned patterns, what the queries actually mean is often not understandable by humans.  
The next matrix used in attention is the key matrix. Like with the query matrix, the key matrix is also multiplied to every vector to produce a list of key vectors. Key vectors are not exactly answers to queries, but more like how relevant the corresponding token to a query asked by another token. This "relevance" value is measured by the dot product of the query vector of one token with the key vector of another token. For example, in the query I gave above, the tokens "quick" and "brown" should have key vectors that have a high dot product with the query vector of "fox". The other tokens should have low dot products, indicating they aren't relevant to being adjectives of "fox". This table of dot products will be used to measure the "relevance" between tokens for that layer.[^7]  
Now, we have a table of dot products for every pair of tokens in the context window. The final step of attention is to figure out how to change the vectors to incorporate contextual meaning. This is calculated with a third matrix, the value matrix. This matrix is multiplied to every token to get the value vectors. Those vectors are then scaled by the dot products and added to the corresponding query tokens in the query-key token pairs. In the example of "quick brown fox", we take the value vectors of "quick" and "brown", multiply them by the dot products of their key vectors with the query vector of "fox", and add it to the embedding vector of "fox". The resulting "fox" vector should also somehow represent that it is quick and brown.
### Final Attention Formula
Unlike the convention in pure linear algebra, where vectors are often represented as columns, transformer models represent each token’s embedding as a row in a matrix.
This aligns more with common practice in machine learning and data science, where each row represents one data sample, in this case, a token, and columns represent features or dimensions. I will only use rows as vectors when discussing the attention formula - columns just make more sense when discussing linear algebra concepts. If we join all the embedded vectors of the context as rows of a matrix, here is the full formula for the shift applied to all entries from attention:  
$$\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$$  
*(Formula taken directly from the article [Attention is All You Need](https://arxiv.org/pdf/1706.03762))*  
A couple of clarifying notes about this formula: the dot products are scaled by $$\frac{1}{\sqrt{d_k}}$$ to prevent value explosion. The softmax function understood to be applied individually to each row of the matrix expression within and is used to get the relevance of each query to each key as a probability distribution. Multiplying a matrix to another matrix's transpose is equivalent to taking the dot product between each matrix's columns represented as vectors. This formula has row $$i$$ representing query vector $$i$$ and column $$j$$ representing key vector $$j$$.
### Multi-Headed Attention
Just like how the same thing can be viewed from multiple lenses, it is often beneficial to run multiple concurrent attention threads in each layer, called multi-headed attention. Each thread is called an attention head, which have their own key, query, and value matrices. After computing the desired changes outputted from each attention head, those values are added together and applied to the current layer of embedded vectors. Since each thread is independent and can be run in parallel, using multiple heads doesn't actually slow down performance that much.
## Knowing Facts
If you have used ChatGPT a lot, you probably have noticed it seems very, very knowledgeable, as if it has the entire internet downloaded in its brain. Because transformers are not trained specifically to store facts, this illusion of being very knowledgeable is actually kind of a mystery, but studies show strong evidence that this phenomenon is most likely occuring in the multi-layer perceptron (MLP), which is just a fancy way of describing a specific kind of neural network within a larger AI model. This MLP consists of an input and output layer of the same size, and one hidden layer that is much larger han the input and output. With transformers, each token goes through an identical copy of the MLP, with the embedded vector entries as the input neurons. In theory, facts are stored within the parameters of the MLP. In reality, what those parameters are actually doing is hard to understnad, like many things in machine learning.
### The Universe of Facts
The feed-forward step from the input to the hidden layer is called the up-projection, because the image goes from a (relatively) small dimension to a larger dimension. In the case of GPT-3, the "fact space" is about four times larger than the dimension space. With vecotrs being columns again, the linear transformation matrix representing this step would have four times as many rows as columns. Imagine each row as sort of a "representative" of a fact. If an embedding vector has a high dot product with the vector represented by the transpose of this row, this should theoretically mean that the corresponding token is relevant to whatever fact is represented by that row. For example, if one row represents the fact "eats rabbits", then the embedding vector of the token "fox" should have a high dot product with that row. So you could view the up-projection as a linear transformation or as computing a list of dot products - both are meaningful interpretations.  
After applying a nonlinear normalization function (usually ReLU for its on/off feature) on the hidden layer, we apply another linear transformation to compute the output layer, just like in a neural network. Since the output layer is the same size as the input layer, which is much smaller than the hidden layer, the linear transformation maps a larger dimension to a smaller dimension, so it is called the down-projection. The corresponding matrix would have many more columns than rows. In this matrix, think of the columns as representing vectors that can imbue facts into an embedding vector. The linear transformation scales each "fact vector" by its corresponding dot product and is added to the vector represented by the output layer. After scaling every fact vector by its corresponding dot product and adding it to the output, we get a vector that should be able to imbue all the relevant facts when added to the embedding vector.
## How Transformers Learn
The learning process for transformers requires assessing how accurately the model predicts the next token. In the case of ChatGPT, the training data is content from the internet, and accuracy of predictions can be evaluated using what token actually came next at each position in the content versus what the model predicted should come next. Also, instead of just adjusting some weights and biases, every entry of every matrix has an impact on the cost function. However, the computation formulas and repetition processes are identical to those in backpropagation from neural networks.  
### Predicting a Token
First, it is important to understand how a transformer computes its prediction. At the end of the attention-MLP sequence, every vector is now richly imbued with factual and contextual information. We take the last embedding vector, representing the last token, and apply a linear transformation using the unembedding matrix. This matrix has a number of rows equal to the number of possible tokens, so it is probably much larger than the dimension of embedding space, or the number of columns. The resulting vector is then softmaxed to produce a probability distribution of all possible next tokens. The softmax function is defined as follows:  
$$p_i=\frac{e^{a_i}}{\sum_{j=1}^n e^{a_j}}$$
### Cost Function
Just like with neural networks finding recommended YouTube videos, transformers are trained by using real-world data and comparing the predicted next tokens with the expected next token at every token position. For large language models, cross-entropy loss is preferred over mean square error to define the cost function. Cross-entropy loss is defined as follows:  
$$
C=-\log{p_k}
$$  
In this equation, $$p_k$$ is the predicted probability of token $$k$$. We specify $$k$$ to be the index of the token that is expected to come next, so the expected value of $$p_k$$ should be 1, while all other $$p_i$$ are 0. While this formula seems to disregard the activations of all other outputs, they are actually indirectly considered through the definition of softmax. See the calculations (where I use the natural log for simplicity):  
$$p_i=\frac{e^{a_i}}{\sum_{j=1}^n e^{a_j}}$$  
$$\text{For all }i\neq k,$$  
$$\frac{\delta C}{\delta a_i}=-\frac{1}{p_k}*\frac{\delta p_k}{\delta a_k}$$  
$$=-\frac{1}{p_k}*\frac{\delta}{\delta a_i}\frac{e^{a_k}}{\sum_{j=1}^n e^{a_j}}$$  
$$=-\frac{1}{p_k}*-\frac{e^{a_k}e^{a_i}}{(\sum_{j=1}^n e^{a_j})^2}\text{(Chain rule and quotient rule)}$$  
$$\text{Recall that }p_i=\frac{e^{a_i}}{\sum_{j=1}^n e^{a_j}}\text{, and factor out a }p_k\text{ from the second fraction}$$  
$$=-\frac{1}{p_k}*p_k*-\frac{e^{a_i}}{\sum_{j=1}^n e^{a_j}}$$  
$$=\frac{e^{a_i}}{\sum_{j=1}^n e^{a_j}}$$  
$$=p_i$$  
The derivative the cost function with respect to $$p_k$$ is different than all other $$p_i$$. However, it can also be largely simplified:  
$$\frac{\delta C}{\delta a_i}=-\frac{1}{p_k}*\frac{\delta p_k}{\delta a_k}$$  
$$=-\frac{1}{p_k}*\frac{\delta}{\delta a_k}\frac{e^{a_k}}{\sum_{j=1}^n e^{a_j}}$$  
$$=-\frac{1}{p_k}*\frac{(e^{a_k}\sum_{j=1}^n e^{a_j})-e^{2a_k}}{(\sum_{j=1}^n e^{a_j})^2}\text{(Chain rule and quotient rule)}$$  
$$\text{Again, we can factor out a }p_k\text{ from the second fraction}$$  
$$=-\frac{1}{p_k}*p_k*\frac{(\sum_{j=1}^n e^{a_j})-e^{a_k}}{\sum_{j=1}^n e^{a_j}}$$  
$$=-(\frac{\sum_{j=0}^n e^{a_j}}{\sum_{j=0}^n e^{a_j}}-\frac{e^{a_k}}{\sum_{j=0}^n e^{a_j}})$$  
$$=p_k-1$$  
Since any point of a (in this case, discrete) probability distribution can only take values between 0 and 1, we should expect $$p_i \mid i\neq k$$ to always be positive and $$ p_k $$ to always be negative. This makes sense because we always want to increase the probability of the correct prediction and decrease the probability of the incorrect prediction. Additionally, large values of $$p_i\mid i\neq k$$ affect the cost function more since confident incorrect predictions are more alarming and necessitate a more aggressive fix. Similarly, small values of $$p_k$$ have strong influence since confidently classifying the correct token as incorrect is problematic. The intermediary calculations look scary, but it actually simplifies quite a lot. Finding simple formulas like this is essential to optimizing performance of large models such as transformers. With appropriate masking[^7], we can now feed our model data from the internet and run backpropagation using the derived cost function derivatives.
## Transformers Conclusion
Transformers are a key step forward in many machine learning areas such as natural language processing. Building off the foundation of neural networks, the carefully-designed architecture and flexibility of transformers allows it to effectively create the illusion of generating new content, remembering relevant information, and understanding data in context - all from simply predicting the next token. The model is also designed with performance in mind, exploiting matrix multiplications and other independent computations extensively to maximize parallel processing potential.
### Dimensions of the Semantic Universe
What I found interesting learning about transformers the first time was the dimensions. GPT-3 uses ~13k dimensions for the embedding space and ~52k dimensions for the fact space. Despite this being many more dimensions than we can visualize, I initially thought there's no way a machine can accurately capture the semantic universe in a reasonable amount of dimensions, and certiainly not every fact that has ever existed on the internet.  
There is a mathematical explanation to this. To represent independent meanings or facts, we need orthogonal vectors, which won't affect each other when added. While a vector space of $$n$$ dimensions can only contain $$n$$ orthogonal vectors, we can actually pack in more if we allow these vectors to be "nearly orthogonal" - maybe between 89 and 91 degrees from each other. In 3 dimensions, this doesn't give us much freedom to squeeze in more vectors, so it's hard to visualize how this slight wiggle room helps us. But interestingly, the number of nearly orthogonal vectors that can be packed into a vector space grows expnentially in terms of the dimension space. For example, you can fit nearly 100,000 vectors, all between 89 and 91 degrees of each other, using just 100 dimensions!  
Consequently, layers within a transformer rarely have distinct entries, but rather a blur of values, since these "idea vectors" are all somewhat blurred with each other. But it could also be due to how meaning is structured. Our semantic universe doesn't really have any completely independent ideas that scale linearly. There's almost always a way to find a way that two ideas are related. Perhaps this is one aspect that transformers can take advantage of to capture some approximation of all ideas using a limited number of dimensions.  
### Matrices and Dimensions Summary
There are four matrices used in transformers. Here are their dimensions in row x column format:  
$$\text{Query matrix: }c\times e$$  
$$\text{Key matrix: }c\times e$$  
$$\text{Value matrix: }e\times e$$  
$$\text{Fact matrix: }f\times e$$
$$c<e<f$$  
The inequality shows the relations of dimension sizes, where $$c$$ is dimension of the "contextualization space", the vector space used to find meaningful contextual questions and answers. $$e$$ is the "embedding space", the vector space used to define a token's meaning. $$f$$ is the dimension of the "fact space", the vector space that, in theory, somehow stores every fact the transformer appears to know.
# Footnotes
[^1]: Often, parameters are initialized using controlled random values such as with Kaiming He Initialization, to mitigate the gradient explosion problem  
[^2]: While this notation is more intuitive to think about, it may be more convenient to represent the edge as travelling backwards - $${w_l}^{ij}$$ is the weight of the edge connecting layer $$l$$ neuron $$i$$ to layer $$l-1$$ neuron $$j$$ - for the purposes of computation  
[^3]: As you will see in the transformers section, mean square error is not the only way to define a cost function
[^4]: A trick called stochastic gradient descent is often employed to significantly improve computation cost for only a marginal accuracy loss  
[^5]: There is a lot more to say about how the training process is conducted, such as techniques to prevent overfitting, but those discussions are more within the data science field
[^6]: Tokens do not necessarily represent full words - they might represent prefixes or suffixes like pre- or -tion. However, it is simpler to conceptually think about tokens as words  
[^7]: In the case of large language models, it is often helpful to set all entries representing query tokens that came before key tokens to zero, called masking. This prevents the transformer from "cheating" by referencing future content during the training process