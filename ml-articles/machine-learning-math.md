---
layout: default
title: "Machine Learning: With Math"
permalink: /machine-learning-math/
---
# Table of Contents
- [Neural Networks](#neural-networks)
    - [Thinking](#thinking)
        - [Feed-Forward](#feed-forward)
        - [Multiple Layers](#multiple-layers)
    - [Learning](#learning)
        - [Backpropagation](#backpropagation)
        - [Iterating Backwards](#iterating-backwards)
    - [Memorizing Without Learning](#memorizing-without-learning)
        - [Early Stopping](#early-stopping)
        - [Dropout](#dropout)
        - [Parameter Regularization](#parameter-regularization)
        - [Data Augmentation](#data-augmentation)
        - [Network Complexity](#network-complexity)
    - [Neural Networks Conclusion](#neural-networks-conclusion)
- [Transformers](#transformers)
    - [Words to Numbers](#words-to-numbers)
        - [Embedding Vectors](#embedding-vectors)
    - [Pay Attention](#pay-attention)
        - [Query Matrix](#query-matrix)
        - [Key Matrix](#key-matrix)
        - [Value Matrix](#value-matrix)
        - [Final Attention Formula](#final-attention-formula)
        - [Multi-Headed Attention](#multi-headed-attention)
    - [Knowing Facts](#knowing-facts)
        - [Up-Projection](#up-projection)
        - [Down-Projection](#down-projection)
    - [How Transformers Learn](#how-transformers-learn)
        - [Predicting a Token](#predicting-a-token)
        - [Cost Function](#cost-function)
    - [Transformers Conclusion](#transformers-conclusion)
        - [Dimensions of Matrices](#dimensions-of-matrices)
        - [This Actually Works?](#this-actually-works)

# Neural Networks
Neural networks were one of the first examples of machine learning (besides maybe linear regression), and serves as a foundation for most modern models. What makes this data structure "neural" is its flexibility. A neural net is initialized with random[^1], arbitrary parameters which can be adjusted through applying a learning process on training data. The edges (represented as lines) connecting every pair of neurons (represented as circles) in adjacent layers allow information to be passed to and interact with each other. In the diagram below, while there are four neurons in the first layer and two neurons in the last layer, the size and number of layers can vary depending on the application.

![Neural Network Diagram](/assets/images/neural_network.png)  
*(Image credit: [Victor Zhou â€“ Neural Networks from Scratch](https://victorzhou.com/series/neural-networks-from-scratch/))*

## Thinking
*"Machine thinking is just linear algebra on steroids." - Someone*  
Before getting into how a neural net learns, let's first discuss how it thinks. In the diagram above, imagine each edge connecting pairs of neurons as having a value associated with it, which we call its weight. Before the neural net looks at anything, the neurons themselves are empty, but they have an offset value, called its bias. These terms should become more clear as I explain the thinking process, which is called feed-forward.
### Feed-Forward
Now we are ready to give the neural net something to think about. In this section, I will use the classic example of recognizing handwritten digits. To pass an image into a neural net, it must somehow be converted into a list of numbers, or a vector. If the image is grayscale, you might convert each pixel into its darkness level. With an input represented as a vector, it can be passed into the first layer of the neural net, called the input layer. The diagram above uses 4 input neurons, but for our case, if the image is, for example, 28x28 pixels, we would use 784 input neurons. Next, for every neuron in the next layer, we calculate its actviation by summing the products of every value in the input layer and the associated weight of the edge that connects it to the next neuron.  
That was a lot of words! But if you think carefully about the math process actually happening here, this is essentially a matrix mutliplication, or a linear transformation. Once we get the activations of every neuron in the next layer, we offset each value by the corresponding bias. Finally, each value is sent through a nonlinear normalization function, such as sigmoid or ReLU. The purpose of this nonlinear function is to keep activations meaningful and improve stability. The choice of function depends on the application and each function has its benefits and drawbacks. (Data scientists have a lot more to say about this, but I will move on) The function used in each layer can also be different. For some applications, such as a handwritten digit recognizer, the softmax function is used in the output layer to convert the activations to a probability distribution.  
Nonlinear functions also bring in more flexibility by breaking properties of linear transformations. You may have thought that using an input size smaller than the hidden layer size, as shown in the diagram, is wasteful since the rank of the first linear transformation can be at most the size of the input layer. By applying nonlinear normalization functions to the first hidden layer, we allow the dimension of the transformation image to become nonlinear and potentially have more dimensions.
### Multiple Layers
We can now repeat this process for the next layers, until we reach the last layer, the output layer. The final values in the output layer determine what the neural net "thinks" about its input. In our case of handrwitten digits, we would use 10 neurons, each with an activation that represents how close the input is to that number. This process is essentially a repeated sequence of a linear transformation, a translation, then a nonlinear normalization function. Here's the full formula for the activation of each neuron:  
$${a_l}^i=N(b_l^i+\sum_{j=1}^n{a_{l-1}^j}w_{l-1}^{ij})$$  
Where $${a_l}^i$$ is the activation of neuron $$i$$ in layer $$l$$, $$N$$ is the nonlinear normalization function, $$n$$ is the size of the current layer, $${w_l}^{ij}$$ is the weight of the edge connecting layer $$l$$ neuron $$i$$ to layer $$l+1$$ neuron $$j$$[^2], and $$b_l^i$$ is the bias for layer $$l$$ neuron $$i$$. (superscripts here are just indices, not exponents)  
You might be wondering why we use multiple layers. The idea behind the neural network is that each layer should recognize some pattern, maybe a straight line or a curve, then recognize compounded patterns in later layers, maybe a loop or intersection. Using nonlinear normalization functions on each layer allows the neural net to focus on these patterns more. I say in theory because the patterns a neural net actually learns is very hard to deduce. There are actually many ways a machine can learn to recognize the number 4, most of which are not recognizable to us.
## Learning
*"Machine learning is just multivariable calculus on steroids." - Someone*  
As I mentioned before, all parameters, meaning the weights and biases, are initialized to random values. As you would expect, a newborn neural net would not make very accrurate predictions - it is literally blind guessing. We want a way to shift those weights and biases to make the predictions more accurate.
### Defining the Function
The first step is designing a mathematical function to tell the neural net how "badly" it predicted. We call this the cost function. For our example, if we input a vector representation of the handwritten digit 7, we should see the neuron corresponding to 7 be set to 1 and all the others set to 0. We therefore can define the cost function as the mean squared error between the predicted value and expected value[^3]. Formally, it is denoted as:  
$$C=\sum_{i=1}^n{(a_l^i-y^i)^2}$$  
Where $$n$$ is the size of the output layer, $$a_l^i$$ is the predicted activation of neuron $$i$$, and $$y^i$$ is the expected activation of neuron $$i$$. With a well-defined function, we can compute the gradient derivative to find which direction will decrease the cost function the fastest. Think of it like finding the fastest descent to the bottom of a high-dimensional elevation field. However, is a problem with directly computing the derivative. Notice how the activation of each output neuron is a sum of functions of previous activations, which in turn are also sums of functions of previous activations. Trying to expand out these sums will become exponentially massive. Not remotely practical for deep neural nets. Instead, we use an algorithm called backpropagation, which allows us to iteratively apply the chain rule and optimize the computation.
### Backpropagation
![Simplified Neural Network](/assets/images/nn_simple.png)  
To illustrate backpropagation, let's simplify our neural network to just three layers (1 input, 1 hidden, 1 output), with just one neuron in each layer. Since we are temporarily ignoring the presence of multiple neurons in a layer, we do not need the superscript yet.  Focusing on just the last two layers, let's first compute the derivative of the cost function with respect to the activation of the output neuron:  
$$\frac{\delta C}{\delta a_l}=2(a_l-y)$$  
While we cannot directly change an activation, this formula will be important to determine the deriatives of other parameters that can induce a change in the activation, thus changing the cost: change the weight of the edge connecting the previous neuron to the output, change the bias applied to the output neuron, or indirectly change the activation of the previous neuron. We can apply the chain rule to compute the derivative of the cost function with respect to each of these variables:  
$$\frac{\delta C}{\delta w_{l-1}}=2(a_l-y)\frac{\delta a_l}{\delta w_{l-1}}$$  
$$\frac{\delta C}{\delta b_l}=2(a_l-y)\frac{\delta a_l}{\delta b_l}$$  
$$\frac{\delta C}{\delta a_{l-1}}=2(a_l-y)\frac{\delta a_l}{\delta a_{l-1}}$$  
As you can see, all the derivatives are scaled by how far away the output neuron activation is from the expected value. That implies that changing outputs that were more badly predicted has a greater impact on the cost. This should make sense. Let's compute the derivative of the output neuron activation with respect to the edge weight:  
$$\frac{\delta a_l}{\delta w_{l-1}}=N'(b_l+a_{l-1}w_{l-1})\frac{\delta}{\delta w_{l-1}}(b_l+a_{l-1}w_{l-1})$$  
$$=N'(b_l+a_{l-1}w_{l-1})a_{l-1}$$  
Next, derivative of the output neuron activation with respect to the bias:  
$$\frac{\delta a_l}{\delta b_l}=N'(b_l+a_{l-1}w_{l-1})\frac{\delta}{\delta b_l}(b_l+a_{l-1}w_{l-1})$$  
$$=N'(b_l+a_{l-1}w_{l-1})$$  
Finally, derivative of the output neuron activation with respect to the activation of the previous neuron:  
$$\frac{\delta a_l}{\delta a_{l-1}}=N'(b_l+a_{l-1}w_{l-1})\frac{\delta}{\delta a_{l-1}}(b_l+a_{l-1}w_{l-1})$$  
$$=N'(b_l+a_{l-1}w_{l-1})w_{l-1}$$
### Iterating Backwards
Remember, we cannot use this derivative to directly change the activation of the previous neuron. Instead, we iterate to the previous layer recomputing the partial derivatives for the that layer, using the value of $$\frac{\delta C}{\delta a_{l-1}}=2(a_l-y)\frac{\delta a_l}{\delta a_{l-1}}$$ which we have now computed. Keep iterating backwards until we reach the input layer. At this point, we should have a derivative for every weight and bias calculated. This is our (massive) gradient derivative.  
The partial derivatives for a full neural net with multiple training data points are actually not much more complicated. We simply need to throw the summations and superscripts back in. You can verify these expressions on your own:  
$$\frac{\delta C}{\delta a_l^i}=2(a_l^i-y^i)$$  
$$\frac{\delta C}{\delta w_{l-1}^{ij}}=2(a_l^i-y^i)N'(b_l+\sum_{j=1}^{n}a_{l-1}^iw_{l-1}^{ij})a_{l-1}^j$$  
$$\frac{\delta C}{\delta b_l^i}=2(a_l^i-y^i)N'(b_l^i+\sum_{j=1}^{n}a_{l-1}^iw_{l-1}^{ij})$$  
$$\frac{\delta C}{\delta a_{l-1}^j}=\sum_{i=1}^{n}2(a_l^i-y^i)N'(b_l^i+\sum_{j=1}^{n}a_{l-1}^iw_{l-1}^{ij})w_{l-1}^{ij}$$  
The main thing to note here is that for every neuron activation in a layer, its activation will be influenced by every neuron activation in the previous layer, so we need to sum up those derivatives to compute $$\frac{\delta C}{\delta a_{l-1}}$$. For multiple training data points, simply sum all[^4] the gradients together to get the average change to optimally fit all outputs. With the full gradient derivative computed, we now shift every weight and bias by a tiny fraction of its corresponding derivative value. This should inch us slightly closer to a better predictor. There's more to say about how parameters get updated for optimal long-term improvement, but after repeating this process over and over on the training data, we should have a pretty solid handwritten digit recognizer.
## Memorizing Without Learning
A problem I encountered when building my own neural network was that while the cost appeared to have decreased to nearly zero, it still did barely better than blind guessing on handwritten digits I drew myself. This problem is called overfitting - where the weights and biases have been adjusted to match the training data perfectly, but performs poorly on new data.

![Overfitting Diagram](/assets/images/overfitting.png)  
*(Image credit: [Shiv Vignesh - The Perfect Fit for a DNN.](https://medium.com/analytics-vidhya/the-perfect-fit-for-a-dnn-596954c9ea39))*

The diagram above illustrates the overfitting problem. A model that closely follows the training data does not necessarily learn the actual patterns. While simply using more training data could help, this requires more time and energy for the model and data is not always easily accessible. There are several regularization techniques used to reduce overfitting. The idea behind most of them is to try and force the network to learn the true patterns, not just random noise within the training data.
### Early Stopping
Perhaps the simplest way to reduce overfitting is to stop training before overfitting occurs. We let the model pick up on the major patterns but stop before it starts learning false patterns that are simply caused by data noise. While this technique is simple, it requires careful analysis to determine when the actual performance of the model on new data is the best. Sometimes, we may not achieve the desired accuracy before overfitting kicks in.
### Dropout
Another popular technique is called dropout. During the training process, dropout will randomly select neurons to be deactivated, meaning they won't have their activation passed forward. Common dropout rates for neurons are between 20-50%, though the optimal rate depends on the model and application. Of course, we want to preserve the output activations, and during evaluation of new data, all neurons are active. Dropout prevents the network from being over-reliant on certain neurons and forces the network to learn more robust patterns.
### Parameter Regularization
Parameter Regularization is similar to how dropout tries to make the network less reliant on specific neurons. In this case, we focus on the weights. A well-trained model should not have a few super large weights to encourage generalization. Parameter regularization is usually not applied to biases because biases are more like offsets, which aren't as prone to cause overfitting as weights. L1 regularization has every edge between neurons contribute the magnitude of its weight (or some multiple of it) to the cost function. L2 regularization has every edge contribute the square of the magnitude of its weight to the cost function, so reducing weights with larger magnitudes are prioritized more in the gradient descent. Often, a combination of L1 and L2 are used to strike an optimal balance of their effects. Since these terms are independent of all the backpropagation calculations, we simply need to add the following terms to every weight in the gradient descent for L1 or L2 regularization, respectively:  
$$
C'(l, i, j)=
\begin{cases}
1,  & \text{if $w_l^{ij} \leq$ 0} \\
-1, & \text{if $w_l^{ij}$ > 0}
\end{cases}
$$  
$$C'(l, i, j)=-2w_l^{ij}$$  
### Data Augmentation
When large datasets are inaccessible, we can create our own data using data augmentation. Using variants of training data on each iteration of backpropagation prevents the model from overfitting to the data. In the case of recognizing handwritten digits, we might translate, rotate, or dilate the position of every pixel to mimic different writing styles. For computational simplicity, a 2x2 linear transformation matrix can be used to generate these new image variants. To keep the result similar to the original, the (1, 1)th and (2, 2)th entry would be close to 1, while the (1, 2)th and (2, 1)th entry would be close to 0:  
$$\begin{pmatrix}\text{Rand}[0.9,1.1] & \text{Rand}[-0.1,0.1]\\\ \text{Rand}[-0.1,0.1] & \text{Rand}[0.9,1.1]\end{pmatrix}$$  
Another way to augment data is adding noise. Noise, which is small random values unrelated to the pattern, is already present in most real-world datasets. Neural nets can sometimes learn the noise patterns rather than the actual patterns in the dataset. To counteract this, we can add artificial noise that changes on each iteration of backpropagation to force the model to learn the true patterns in the dataset. In the case of recognizing handwritten digits, we might randomly increase or decrease the brightness of every pixel by a tiny amount.
### Network Complexity
A neural net without enough parameters may not be able to fit to the patterns in the dataset. On the other hand, a more complex neural net may not be better. Models with too many neurons in each layer or too many layers has too much flexibility to learn excessively complex patterns that aren't related to the training data. In other words, the model has too many parameters to work with and has a greater ability to simply memorize the training data. By adjusting the size and number of layers, the optimal network complexity can be discovered empirically.
## Neural Networks Conclusion
With neural networks, we have a unique data structure with flexible parameters, allowing it to fit to any evaluation application. The combination of linear transformations and nonlinear normalization functions preserve both computational simplicity and the ability to recognize nonlinear patterns. Having a well-defined function for output values allows a derivative gradient to be calculated, illuminating the necessary changes to adjust the parameters to better fit training data. However, it is essential to manually analyze the model to find optimal network design, function choice, and prevent overfitting.
# Transformers
While neural networks are great at multiple choice, (evaluation tasks such as recognizing handwritten digits) they struggle to generate content. Transformers were proposed to solve this task. By predicting the next word, pixel, or wave (we will call these segments of content "tokens") to come next, transformers give the illusion of original generation. They are used for modern tasks such as natural language processing, computer vision, reinforcement learning, and other tasks that are simply too complex for neural networks alone to be practical. A popular example is ChatGPT (GPT stands for Generative Pre-trained Transformer), which can mimic conversations using data from the internet.  
Interestingly, transformers were not the first machine learning architecture proposed for these tasks. Other models, such as recurrent neural networks, have been used, but transformers are have higher performance and more parallelizable, cutting training and computing time cost. Transformers use multiple layers to allow for deeper learning, just like neural networks, but what these layers do is more complex. In this section, I will mainly base my discussion around the model that powers ChatGPT as it is a very popular example of a transformer.
## Words to Numbers
Computers love programming languages, but have a hard time understanding natural languages. Language is imprecise, has culture weaved into it, and can change meaning depending on the context. The word "queen", for example, can represent a royal figure or a chess piece. Adjectives before nouns can also drastically change meaning. In short, it is difficult, if not impossible, to convert natural language to a language computers can understand.
### Embedding Vectors
Nevertheless, we can approximate meanings. We want to convert words to tokens[^5], which have associated embedding vectors. These vectors have very high dimension (>10,000 in GPT-3) because we want to be able to somehow capture the entire semantic universe into a list of numbers. Imagine a high-dimensional space (which I like to call the semantic universe) where each axis represents an idea. Maybe one axis is the "color" axis, where embedding vectors of words that are associated with different colors have different entries in that dimensions. Another axis could be the "temperature" axis, another being the "agreeable" axis, and so on, until we can approximate every point in the semantic universe with a vector. However, just like how the patterns a neural network learns is rarely understandable to humans, the meanings of each dimension that a transformer learns usually does not makes sense.  
Another property of these embedding vectors is that vectors can be added together to represent compounded ideas. Suppose $$v_1$$ represents the embedding vector associated with the word "mother", $$v_2$$ represents "father", $$v_3$$ represents "uncle", and $$v_4$$ represents "aunt". $$v_1-v_2$$, the difference between "mother" and "father", is very close to $$v_3-v_4$$, the difference between "uncle" and "aunt". This suggests that the vector $$v_1-v_2$$ can somehow convert masculine words to their feminine counterpart when added to an embedding vector. It also suggests that our complex semantic universe, despite having far more dimensions than we can visualize, is somehow quantifiable with linear algebra? And how could a mere 10,000 dimensions possibly achieve this? I found this strange and slightly unsettling when I learned about it. I'll return to this point at the end of the section.
## Pay Attention
A major difficulty with conversational bots is its ability to remember the past. Transformers take advantage of a revolutionary algorithm to remember context proposed by the paper *Attention is All You Need*, called attention. While recurrent neural networks were able to remember context, its processing mechanism was sequential, unable to be parallelized, and had very unstable gradients in the training process. Attention mitigates these issues, making it faster and more accurate for larger context sizes.  
Visualize the input as a list of embedding vectors, representing a text sequence. For example, there might be 8 vectors representing the incomplete sentence "The quick brown fox jumps over the lazy". Attention assumes that every token could have some sort of influence on every other token, so we want to compute what those influences are and how to adjust the vectors to include its more refined contextual meaning. We create an nxn attention matrix to compare every pair of tokens. We then use three matrices to determine how their embedded vector entries should be updated.
### Query Matrix
The first of these matrices is the query matrix. This matrix is multiplied to every vector to produce a list of query vectors. Think of query vectors as a question that we want the corresponding token to ask of its context. In the example sentence, the token "fox" might have the query "are there any adjectives that change my definition?" The query matrix should be learned to produce these useful questions when applied to embedded vectors. Like with most machine-learned patterns, what the queries actually mean is often not understandable by humans.
### Key Matrix
The next matrix used in attention is the key matrix. Like with the query matrix, the key matrix is also multiplied to every vector to produce a list of key vectors. Key vectors are not exactly answers to queries, but more like how relevant the corresponding token to a query asked by another token. This "relevance" value is measured by the dot product of the query vector of one token with the key vector of another token. For example, in the query I gave above, the tokens "quick" and "brown" should have key vectors that have a high dot product with the query vector of "fox". The other tokens should have low dot products, indicating they aren't relevant to being adjectives of "fox". This table of dot products will be used to measure the "relevance" between tokens for that layer.[^6]
### Value Matrix
Now, we have a table of dot products for every pair of tokens in the context window. The final step of attention is to figure out how to change the vectors to incorporate contextual meaning. This is calculated with a third matrix, the value matrix. This matrix is multiplied to every token to get the value vectors. Those vectors are then scaled by the dot products and added to the corresponding query tokens in the query-key token pairs. In the example of "quick brown fox", we take the value vectors of "quick" and "brown", multiply them by the dot products of their key vectors with the query vector of "fox", and add it to the embedding vector of "fox". The resulting "fox" vector should also somehow represent that it is quick and brown.
### Final Attention Formula
Unlike the convention in pure linear algebra, where vectors are often represented as columns, transformer models represent each tokenâ€™s embedding as a row in a matrix.
This aligns more with common practice in machine learning and data science, where each row represents one data sample, in this case, a token, and columns represent features or dimensions. I will only use rows as vectors when discussing the attention formula - columns just make more sense when discussing linear algebra concepts. If we join all the embedded vectors of the context as rows of a matrix, here is the full formula for the shift applied to all entries from attention:  
$$\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$$  
*(Formula taken directly from the article [Attention is All You Need](https://arxiv.org/pdf/1706.03762))*  
A couple of clarifying notes about this formula: the dot products are scaled by $$\frac{1}{\sqrt{d_k}}$$ to prevent value explosion. The softmax function understood to be applied individually to each row of the matrix expression within and is used to get the relevance of each query to each key as a probability distribution. Multiplying a matrix to another matrix's transpose is equivalent to taking the dot product between each matrix's columns represented as vectors. This formula has row $$i$$ representing query vector $$i$$ and column $$j$$ representing key vector $$j$$.
### Multi-Headed Attention
Just like how the same thing can be viewed from multiple lenses, it is often beneficial to run multiple concurrent attention threads in each layer, called multi-headed attention. Each thread is called an attention head, which have their own key, query, and value matrices. After computing the desired changes outputted from each attention head, those values are added together and applied to the current layer of embedded vectors. Since each thread is independent and can be run in parallel, using multiple heads doesn't actually slow down performance that much.
## Knowing Facts
If you have used ChatGPT a lot, you probably have noticed it seems very, very knowledgeable, as if it has the entire internet downloaded in its brain. Because transformers are not trained specifically to store facts, this illusion of being very knowledgeable is actually kind of a mystery, but studies show strong evidence that this phenomenon is most likely occuring in the multi-layer perceptron (MLP), which is just a fancy way of describing a specific kind of neural network within a larger AI model. This MLP consists of an input and output layer of the same size, and one hidden layer that is much larger han the input and output. With transformers, each token goes through an identical copy of the MLP, with the embedded vector entries as the input neurons. In theory, facts are stored within the parameters of the MLP. In reality, what those parameters are actually doing is hard to understnad, like many things in machine learning.
### Up-Projection
The feed-forward step from the input to the hidden layer is called the up-projection, because the image goes from a (relatively) small dimension to a larger dimension. In the case of GPT-3, the "fact space" is about four times larger than the dimension space. With vecotrs being columns again, the linear transformation matrix representing this step would have four times as many rows as columns. Imagine each row as sort of a "representative" of a fact. If an embedding vector has a high dot product with the vector represented by the transpose of this row, this should theoretically mean that the corresponding token is relevant to whatever fact is represented by that row. For example, if one row represents the fact "eats rabbits", then the embedding vector of the token "fox" should have a high dot product with that row. So you could view the up-projection as a linear transformation or as computing a list of dot products - both are meaningful interpretations.
### Down-Projection
After applying a nonlinear normalization function (usually ReLU for its on/off feature) on the hidden layer, we apply another linear transformation to compute the output layer, just like in a neural network. Since the output layer is the same size as the input layer, which is much smaller than the hidden layer, the linear transformation maps a larger dimension to a smaller dimension, hence the name down-projection. The corresponding matrix would have many more columns than rows. In this matrix, think of the columns as representing vectors that can imbue facts into an embedding vector. The linear transformation scales each "fact vector" by its corresponding dot product and is added to the vector represented by the output layer. After scaling every fact vector by its corresponding dot product and adding it to the output, we get a vector that should be able to imbue all the relevant facts when added to the embedding vector.
## How Transformers Learn
The learning process for transformers requires assessing how accurately the model predicts the next token. In the case of ChatGPT, the training data is content from the internet, and accuracy of predictions can be evaluated using what token actually came next at each position in the content versus what the model predicted should come next. However, the computation formulas and repetition processes are identical to those in backpropagation from neural networks.  
### Predicting a Token
First, it is important to understand how a transformer computes its prediction. At the end of the attention-MLP sequence, every vector is now richly imbued with factual and contextual information. We take the last embedding vector, representing the last token, and apply a linear transformation using the unembedding matrix. This matrix has a number of rows equal to the number of possible tokens, so it is probably much larger than the dimension of embedding space, or the number of columns. The resulting vector is then softmaxed to produce a probability distribution of all possible next tokens.
### Cost Function
Just like with recognizing handwritten digits using neural networks, transformers are trained by using real-world data and comparing the predicted next tokens with the expected next token at every token position. For large language models, cross-entropy loss is preferred over mean square error to define the cost function. Cross-entropy loss is defined as follows:  
$$
C=-\log{p_i}
$$
$$p_i$$ is the predicted probability of token $$i$$. We specify $$i$$ to be the index of the token that is expected to come next. While this formula seems to disregard the activations of all other outputs, they are actually indirectly considered through the definition of softmax. See the calculations (where I use the natural log for simplicity):
$$
p^i=\frac{e^{a^_i}}{\sum_{j=1}^n e^{a_j}}
\frac{\delta C}{\delta p_i}
$$
## Transformers Conclusion
Transformers are a key step forward in many machine learning areas such as natural language processing. Building off the foundation of neural networks, the carefully-designed architecture and flexibility of transformers allows it to effectively create the illusion of generating new content, remembering relevant information, and understanding data in context - all from simply predicting the next token. The model is also designed with performance in mind, exploiting matrix multiplications and other independent computations extensively to maximize parallel processing potential.
### Dimensions of the Semantic Universe
What I found interesting learning about transformers the first time was the dimensions. GPT-3 uses ~13k dimensions for the embedding space and ~52k dimensions for the fact space. Despite this being many more dimensions than we can visualize, I initially thought there's no way a machine can accurately capture the semantic universe in a reasonable amount of dimensions, and certiainly not every fact that has ever existed on the internet.  
There is a mathematical explanation to this. To represent independent meanings or facts, we need orthogonal vectors, which won't affect each other when added. While a vector space of $$n$$ dimensions can only contain $$n$$ orthogonal vectors, we can actually pack in more if we allow these vectors to be "nearly orthogonal" - maybe between 89 and 91 degrees from each other. In 3 dimensions, this doesn't give us much freedom to squeeze in more vectors, so it's hard to visualize how this slight wiggle room helps us. But interestingly, the number of nearly orthogonal vectors that can be packed into a vector space grows expnentially in terms of the dimension space. For example, you can fit nearly 100,000 vectors, all between 89 and 91 degrees of each other, using just 100 dimensions!  
Consequently, layers within a transformer rarely have distinct entries, but rather a blur of values, since these "idea vectors" are all somewhat blurred with each other. But it could also be due to how meaning is structured. Our semantic universe doesn't really have any completely independent ideas that scale linearly. There's almost always a way to find a way that two ideas are related. Perhaps this is one aspect that transformers can take advantage of to capture some approximation of all ideas using a limited number of dimensions.  
### Matrices Summary
# Footnotes
[^1]: Often, parameters are initialized using controlled random values such as with Kaiming He Initialization, to mitigate the gradient explosion problem  
[^2]: While this notation is more intuitive to think about, it may be more convenient to represent the edge as travelling backwards - $${w_l}^{ij}$$ is the weight of the edge connecting layer $$l$$ neuron $$i$$ to layer $$l-1$$ neuron $$j$$ - for the purposes of computation  
[^3]: As you will see in the transformers section, mean square error is not the only way to define a cost function.
[^4]: A trick called stochastic gradient descent is often employed to significantly improve computation cost for only a marginal accuracy loss  
[^5]: Tokens do not necessarily represent full words - they might represent prefixes or suffixes like pre- or -tion. However, it is simpler to conceptually think about tokens as words  
[^6]: In the case of large language models, it is often helpful to set all entries representing query tokens that came before key tokens to zero, called masking. This prevents the transformer from "cheating" by referencing future content during the training process.